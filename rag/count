pencv (L1) -> tutorial_py_table_of_contents_bindings (L2) -> tutorial_py_bindings_basics (L3)/n(Page path)
(1)  > (2) How to extend new modules to Python?
opencv (L1) -> tutorial_py_table_of_contents_ml (L2) -> tutorial_py_kmeans_index (L3) -> tutorial_py_kmeans_opencv (L4)/n(Page path)
(1) Now plot 'A' in red, 'B' in blue, 'centers' in yellowplt.hist(A,256,[0,256],color = 'r')plt.hist(B,256,[0,256],color = 'b')plt.hist(centers,32,[0,256],color = 'y')plt.show() Below is the output we got: > (2) 3. Color Quantization
opencv (L1) -> tutorial_py_table_of_contents_ml (L2) -> tutorial_py_svm_index (L3) -> tutorial_py_svm_opencv (L4)/n(Page path)
(1)  > (2) Goal
opencv (L1) -> tutorial_py_table_of_contents_ml (L2) -> tutorial_py_svm_index (L3) -> tutorial_py_svm_opencv (L4)/n(Page path)
(1)  > (2) OCR of Hand-written Digits
opencv (L1) -> tutorial_py_table_of_contents_ml (L2) -> tutorial_py_svm_index (L3) -> tutorial_py_svm_opencv (L4)/n(Page path)
(1) !/usr/bin/env pythonimport cv2 as cvimport numpy as npSZ=20bin\_n = 16 # Number of binsaffine\_flags = cv.WARP\_INVERSE\_MAP|cv.INTER\_LINEARdef deskew(img): m = [cv.moments](../../d3/dc0/group__imgproc__shape.html#ga556a180f43cab22649c23ada36a8a139 "../../d3/dc0/group__imgproc__shape.html#ga556a180f43cab22649c23ada36a8a139")(img) if [abs](../../d1/d10/classcv_1_1MatExpr.html#a30843fc6c148a00f5d300a7f43f3fbdc "../../d1/d10/classcv_1_1MatExpr.html#a30843fc6c148a00f5d300a7f43f3fbdc")(m['mu02']) < 1e-2: return img.copy() skew = m['mu11']/m['mu02'] M = np.float32([[1, skew, -0.5\*SZ\*skew], [0, 1, 0]]) img = [cv.warpAffine](../../da/d54/group__imgproc__transform.html#ga0203d9ee5fcd28d40dbc4a1ea4451983 "../../da/d54/group__imgproc__transform.html#ga0203d9ee5fcd28d40dbc4a1ea4451983")(img,M,(SZ, SZ),flags=affine\_flags) return imgdef hog(img): gx = [cv.Sobel](../../d4/d86/group__imgproc__filter.html#gacea54f142e81b6758cb6f375ce782c8d "../../d4/d86/group__imgproc__filter.html#gacea54f142e81b6758cb6f375ce782c8d")(img, cv.CV\_32F, 1, 0) gy = [cv.Sobel](../../d4/d86/group__imgproc__filter.html#gacea54f142e81b6758cb6f375ce782c8d "../../d4/d86/group__imgproc__filter.html#gacea54f142e81b6758cb6f375ce782c8d")(img, cv.CV\_32F, 0, 1) mag, ang = [cv.cartToPolar](../../d2/de8/group__core__array.html#gac5f92f48ec32cacf5275969c33ee837d "../../d2/de8/group__core__array.html#gac5f92f48ec32cacf5275969c33ee837d")(gx, gy) bins = np.int32(bin\_n\*ang/(2\*np.pi)) # quantizing binvalues in (0...16) bin\_cells = bins[:10,:10], bins[10:,:10], bins[:10,10:], bins[10:,10:] mag\_cells = mag[:10,:10], mag[10:,:10], mag[:10,10:], mag[10:,10:] hists = [np.bincount(b.ravel(), m.ravel(), bin\_n) for b, m in zip(bin\_cells, mag\_cells)] hist = np.hstack(hists) # hist is a 64 bit vector return histimg = [cv.imread](../../d4/da8/group__imgcodecs.html#ga288b8b3da0892bd651fce07b3bbd3a56 "../../d4/da8/group__imgcodecs.html#ga288b8b3da0892bd651fce07b3bbd3a56")([cv.samples.findFile](../../d6/dba/group__core__utils__samples.html#ga3a33b00033b46c698ff6340d95569c13 "../../d6/dba/group__core__utils__samples.html#ga3a33b00033b46c698ff6340d95569c13")('digits.png'),0)if img is None: raise Exception("we need the digits.png image from samples/data here !")cells = [np.hsplit(row,100) for row in np.vsplit(img,50)]# First half is trainData, remaining is testDatatrain\_cells = [ i[:50] for i in cells ]test\_cells = [ i[50:] for i in cells]deskewed = [list(map(deskew,row)) for row in train\_cells]hogdata = [list(map(hog,row)) for row in deskewed]trainData = np.float32(hogdata).reshape(-1,64)responses = np.repeat(np.arange(10),250)[:,np.newaxis]svm = cv.ml.SVM\_create()svm.setKernel(cv.ml.SVM\_LINEAR)svm.setType(cv.ml.SVM\_C\_SVC)svm.setC(2.67)svm.setGamma(5.383)svm.train(trainData, cv.ml.ROW\_SAMPLE, responses)svm.save('svm\_data.dat')deskewed = [list(map(deskew,row)) for row in test\_cells]hogdata = [list(map(hog,row)) for row in deskewed]testData = np.float32(hogdata).reshape(-1,bin\_n\*4)result = svm.predict(testData)[1]mask = result==responsescorrect = np.count\_nonzero(mask)[print](../../df/d57/namespacecv_1_1dnn.html#a43417dcaeb3c1e2a09b9d948e234c366 "../../df/d57/namespacecv_1_1dnn.html#a43417dcaeb3c1e2a09b9d948e234c366")(correct\*100.0/result.size)This particular technique gave me nearly 94% accuracy. You can try different values for various parameters of SVM to check if higher accuracy is possible. Or you can read technical papers on this area and try to implement them. > (2) Additional Resources
opencv (L1) -> tutorial_py_table_of_contents_ml (L2) -> tutorial_py_svm_index (L3) -> tutorial_py_svm_opencv (L4)/n(Page path)
(1) !/usr/bin/env pythonimport cv2 as cvimport numpy as npSZ=20bin\_n = 16 # Number of binsaffine\_flags = cv.WARP\_INVERSE\_MAP|cv.INTER\_LINEARdef deskew(img): m = [cv.moments](../../d3/dc0/group__imgproc__shape.html#ga556a180f43cab22649c23ada36a8a139 "../../d3/dc0/group__imgproc__shape.html#ga556a180f43cab22649c23ada36a8a139")(img) if [abs](../../d1/d10/classcv_1_1MatExpr.html#a30843fc6c148a00f5d300a7f43f3fbdc "../../d1/d10/classcv_1_1MatExpr.html#a30843fc6c148a00f5d300a7f43f3fbdc")(m['mu02']) < 1e-2: return img.copy() skew = m['mu11']/m['mu02'] M = np.float32([[1, skew, -0.5\*SZ\*skew], [0, 1, 0]]) img = [cv.warpAffine](../../da/d54/group__imgproc__transform.html#ga0203d9ee5fcd28d40dbc4a1ea4451983 "../../da/d54/group__imgproc__transform.html#ga0203d9ee5fcd28d40dbc4a1ea4451983")(img,M,(SZ, SZ),flags=affine\_flags) return imgdef hog(img): gx = [cv.Sobel](../../d4/d86/group__imgproc__filter.html#gacea54f142e81b6758cb6f375ce782c8d "../../d4/d86/group__imgproc__filter.html#gacea54f142e81b6758cb6f375ce782c8d")(img, cv.CV\_32F, 1, 0) gy = [cv.Sobel](../../d4/d86/group__imgproc__filter.html#gacea54f142e81b6758cb6f375ce782c8d "../../d4/d86/group__imgproc__filter.html#gacea54f142e81b6758cb6f375ce782c8d")(img, cv.CV\_32F, 0, 1) mag, ang = [cv.cartToPolar](../../d2/de8/group__core__array.html#gac5f92f48ec32cacf5275969c33ee837d "../../d2/de8/group__core__array.html#gac5f92f48ec32cacf5275969c33ee837d")(gx, gy) bins = np.int32(bin\_n\*ang/(2\*np.pi)) # quantizing binvalues in (0...16) bin\_cells = bins[:10,:10], bins[10:,:10], bins[:10,10:], bins[10:,10:] mag\_cells = mag[:10,:10], mag[10:,:10], mag[:10,10:], mag[10:,10:] hists = [np.bincount(b.ravel(), m.ravel(), bin\_n) for b, m in zip(bin\_cells, mag\_cells)] hist = np.hstack(hists) # hist is a 64 bit vector return histimg = [cv.imread](../../d4/da8/group__imgcodecs.html#ga288b8b3da0892bd651fce07b3bbd3a56 "../../d4/da8/group__imgcodecs.html#ga288b8b3da0892bd651fce07b3bbd3a56")([cv.samples.findFile](../../d6/dba/group__core__utils__samples.html#ga3a33b00033b46c698ff6340d95569c13 "../../d6/dba/group__core__utils__samples.html#ga3a33b00033b46c698ff6340d95569c13")('digits.png'),0)if img is None: raise Exception("we need the digits.png image from samples/data here !")cells = [np.hsplit(row,100) for row in np.vsplit(img,50)]# First half is trainData, remaining is testDatatrain\_cells = [ i[:50] for i in cells ]test\_cells = [ i[50:] for i in cells]deskewed = [list(map(deskew,row)) for row in train\_cells]hogdata = [list(map(hog,row)) for row in deskewed]trainData = np.float32(hogdata).reshape(-1,64)responses = np.repeat(np.arange(10),250)[:,np.newaxis]svm = cv.ml.SVM\_create()svm.setKernel(cv.ml.SVM\_LINEAR)svm.setType(cv.ml.SVM\_C\_SVC)svm.setC(2.67)svm.setGamma(5.383)svm.train(trainData, cv.ml.ROW\_SAMPLE, responses)svm.save('svm\_data.dat')deskewed = [list(map(deskew,row)) for row in test\_cells]hogdata = [list(map(hog,row)) for row in deskewed]testData = np.float32(hogdata).reshape(-1,bin\_n\*4)result = svm.predict(testData)[1]mask = result==responsescorrect = np.count\_nonzero(mask)[print](../../df/d57/namespacecv_1_1dnn.html#a43417dcaeb3c1e2a09b9d948e234c366 "../../df/d57/namespacecv_1_1dnn.html#a43417dcaeb3c1e2a09b9d948e234c366")(correct\*100.0/result.size)This particular technique gave me nearly 94% accuracy. You can try different values for various parameters of SVM to check if higher accuracy is possible. Or you can read technical papers on this area and try to implement them. > (2) Exercises
opencv (L1) -> tutorial_py_table_of_contents_ml (L2) -> tutorial_py_knn_index (L3) -> tutorial_py_knn_opencv (L4)/n(Page path)
(1) Save the datanp.savez('knn\_data.npz',train=train, train\_labels=train\_labels)# Now load the datawith np.load('knn\_data.npz') as data: [print](../../df/d57/namespacecv_1_1dnn.html#a43417dcaeb3c1e2a09b9d948e234c366 "../../df/d57/namespacecv_1_1dnn.html#a43417dcaeb3c1e2a09b9d948e234c366")( data.files ) train = data['train'] train\_labels = data['train\_labels'] In my system, it takes around 4.4 MB of memory. Since we are using intensity values (uint8 data) as features, it would be better to convert the data to np.uint8 first and then save it. It takes only 1.1 MB in this case. Then while loading, you can convert back into float32. > (2) OCR of the English Alphabet
opencv (L1) -> tutorial_py_table_of_contents_ml (L2) -> tutorial_py_knn_index (L3) -> tutorial_py_knn_understanding (L4)/n(Page path)
(1)  > (2) kNN in OpenCV
opencv (L1) -> tutorial_py_table_of_contents_calib3d (L2) -> tutorial_py_calibration (L3)/n(Page path)
(1)  > (2) Basics
opencv (L1) -> tutorial_py_table_of_contents_calib3d (L2) -> tutorial_py_calibration (L3)/n(Page path)
(1)  > (2) Code > (3) Setup
opencv (L1) -> tutorial_py_table_of_contents_calib3d (L2) -> tutorial_py_calibration (L3)/n(Page path)
(1)  > (2) Code > (3) Calibration
opencv (L1) -> tutorial_py_table_of_contents_calib3d (L2) -> tutorial_py_calibration (L3)/n(Page path)
(1) undistortmapx, mapy = [cv.initUndistortRectifyMap](../../d9/d0c/group__calib3d.html#ga7dfb72c9cf9780a347fbe3d1c47e5d5a "../../d9/d0c/group__calib3d.html#ga7dfb72c9cf9780a347fbe3d1c47e5d5a")(mtx, dist, None, newcameramtx, (w,h), 5)dst = [cv.remap](../../da/d54/group__imgproc__transform.html#gab75ef31ce5cdfb5c44b6da5f3b908ea4 "../../da/d54/group__imgproc__transform.html#gab75ef31ce5cdfb5c44b6da5f3b908ea4")(img, mapx, mapy, cv.INTER\_LINEAR)# crop the imagex, y, w, h = roidst = dst[y:y+h, x:x+w][cv.imwrite](../../d4/da8/group__imgcodecs.html#gabbc7ef1aa2edfaa87772f1202d67e0ce "../../d4/da8/group__imgcodecs.html#gabbc7ef1aa2edfaa87772f1202d67e0ce")('calibresult.png', dst) Still, both the methods give the same result. See the result below: > (2) Re-projection Error
opencv (L1) -> tutorial_py_table_of_contents_calib3d (L2) -> tutorial_py_calibration (L3)/n(Page path)
(1) undistortmapx, mapy = [cv.initUndistortRectifyMap](../../d9/d0c/group__calib3d.html#ga7dfb72c9cf9780a347fbe3d1c47e5d5a "../../d9/d0c/group__calib3d.html#ga7dfb72c9cf9780a347fbe3d1c47e5d5a")(mtx, dist, None, newcameramtx, (w,h), 5)dst = [cv.remap](../../da/d54/group__imgproc__transform.html#gab75ef31ce5cdfb5c44b6da5f3b908ea4 "../../da/d54/group__imgproc__transform.html#gab75ef31ce5cdfb5c44b6da5f3b908ea4")(img, mapx, mapy, cv.INTER\_LINEAR)# crop the imagex, y, w, h = roidst = dst[y:y+h, x:x+w][cv.imwrite](../../d4/da8/group__imgcodecs.html#gabbc7ef1aa2edfaa87772f1202d67e0ce "../../d4/da8/group__imgcodecs.html#gabbc7ef1aa2edfaa87772f1202d67e0ce")('calibresult.png', dst) Still, both the methods give the same result. See the result below: > (2) Exercises
opencv (L1) -> tutorial_py_table_of_contents_calib3d (L2) -> tutorial_py_epipolar_geometry (L3)/n(Page path)
(1)  > (2) Code
opencv (L1) -> tutorial_py_table_of_contents_calib3d (L2) -> tutorial_py_epipolar_geometry (L3)/n(Page path)
(1) Find epilines corresponding to points in right image (second image) and# drawing its lines on left imagelines1 = [cv.computeCorrespondEpilines](../../d9/d0c/group__calib3d.html#ga19e3401c94c44b47c229be6e51d158b7 "../../d9/d0c/group__calib3d.html#ga19e3401c94c44b47c229be6e51d158b7")(pts2.reshape(-1,1,2), 2,F)lines1 = lines1.reshape(-1,3)img5,img6 = drawlines(img1,img2,lines1,pts1,pts2)# Find epilines corresponding to points in left image (first image) and# drawing its lines on right imagelines2 = [cv.computeCorrespondEpilines](../../d9/d0c/group__calib3d.html#ga19e3401c94c44b47c229be6e51d158b7 "../../d9/d0c/group__calib3d.html#ga19e3401c94c44b47c229be6e51d158b7")(pts1.reshape(-1,1,2), 1,F)lines2 = lines2.reshape(-1,3)img3,img4 = drawlines(img2,img1,lines2,pts2,pts1)plt.subplot(121),plt.imshow(img5)plt.subplot(122),plt.imshow(img3)plt.show() Below is the result we get: > (2) Exercises
opencv (L1) -> tutorial_py_table_of_contents_calib3d (L2) -> tutorial_py_pose (L3)/n(Page path)
(1)  > (2) Basics > (3) Render a Cube
opencv (L1) -> tutorial_py_table_of_contents_photo (L2) -> tutorial_py_hdr (L3)/n(Page path)
(1)  > (2) Goal
opencv (L1) -> tutorial_py_table_of_contents_photo (L2) -> tutorial_py_hdr (L3)/n(Page path)
(1)  > (2) Theory
opencv (L1) -> tutorial_py_table_of_contents_photo (L2) -> tutorial_py_hdr (L3)/n(Page path)
(1)  > (2) Exposure sequence HDR > (3) 1. Loading exposure images into a list
opencv (L1) -> tutorial_py_table_of_contents_photo (L2) -> tutorial_py_hdr (L3)/n(Page path)
(1) Merge exposures to HDR imagemerge\_debevec = [cv.createMergeDebevec](../../d6/df5/group__photo__hdr.html#gaa8eab36bc764abb2a225db7c945f87f9 "../../d6/df5/group__photo__hdr.html#gaa8eab36bc764abb2a225db7c945f87f9")()hdr\_debevec = merge\_debevec.process(img\_list, times=exposure\_times.copy())merge\_robertson = [cv.createMergeRobertson](../../d6/df5/group__photo__hdr.html#ga460d4a1df1a7e8cdcf7445bb87a8fb78 "../../d6/df5/group__photo__hdr.html#ga460d4a1df1a7e8cdcf7445bb87a8fb78")()hdr\_robertson = merge\_robertson.process(img\_list, times=exposure\_times.copy())### 3. Tonemap HDR image
opencv (L1) -> tutorial_py_table_of_contents_photo (L2) -> tutorial_py_hdr (L3)/n(Page path)
(1) Tonemap HDR imagetonemap1 = [cv.createTonemap](../../d6/df5/group__photo__hdr.html#gabcbd653140b93a1fa87ccce94548cd0d "../../d6/df5/group__photo__hdr.html#gabcbd653140b93a1fa87ccce94548cd0d")(gamma=2.2)res\_debevec = tonemap1.process(hdr\_debevec.copy())### 4. Merge exposures using Mertens fusion
opencv (L1) -> tutorial_py_table_of_contents_photo (L2) -> tutorial_py_hdr (L3)/n(Page path)
(1) Exposure fusion using Mertensmerge\_mertens = [cv.createMergeMertens](../../d6/df5/group__photo__hdr.html#ga79d59aa3cb3a7c664e59a4b5acc1ccb6 "../../d6/df5/group__photo__hdr.html#ga79d59aa3cb3a7c664e59a4b5acc1ccb6")()res\_mertens = merge\_mertens.process(img\_list)### 5. Convert to 8-bit and save
opencv (L1) -> tutorial_py_table_of_contents_photo (L2) -> tutorial_py_hdr (L3)/n(Page path)
(1) Convert datatype to 8-bit and saveres\_debevec\_8bit = np.clip(res\_debevec\*255, 0, 255).astype('uint8')res\_robertson\_8bit = np.clip(res\_robertson\*255, 0, 255).astype('uint8')res\_mertens\_8bit = np.clip(res\_mertens\*255, 0, 255).astype('uint8')[cv.imwrite](../../d4/da8/group__imgcodecs.html#gabbc7ef1aa2edfaa87772f1202d67e0ce "../../d4/da8/group__imgcodecs.html#gabbc7ef1aa2edfaa87772f1202d67e0ce")("ldr\_debevec.jpg", res\_debevec\_8bit)[cv.imwrite](../../d4/da8/group__imgcodecs.html#gabbc7ef1aa2edfaa87772f1202d67e0ce "../../d4/da8/group__imgcodecs.html#gabbc7ef1aa2edfaa87772f1202d67e0ce")("ldr\_robertson.jpg", res\_robertson\_8bit)[cv.imwrite](../../d4/da8/group__imgcodecs.html#gabbc7ef1aa2edfaa87772f1202d67e0ce "../../d4/da8/group__imgcodecs.html#gabbc7ef1aa2edfaa87772f1202d67e0ce")("fusion\_mertens.jpg", res\_mertens\_8bit)## Results > (2)  > (3) Debevec:
opencv (L1) -> tutorial_py_table_of_contents_photo (L2) -> tutorial_py_hdr (L3)/n(Page path)
(1) Convert datatype to 8-bit and saveres\_debevec\_8bit = np.clip(res\_debevec\*255, 0, 255).astype('uint8')res\_robertson\_8bit = np.clip(res\_robertson\*255, 0, 255).astype('uint8')res\_mertens\_8bit = np.clip(res\_mertens\*255, 0, 255).astype('uint8')[cv.imwrite](../../d4/da8/group__imgcodecs.html#gabbc7ef1aa2edfaa87772f1202d67e0ce "../../d4/da8/group__imgcodecs.html#gabbc7ef1aa2edfaa87772f1202d67e0ce")("ldr\_debevec.jpg", res\_debevec\_8bit)[cv.imwrite](../../d4/da8/group__imgcodecs.html#gabbc7ef1aa2edfaa87772f1202d67e0ce "../../d4/da8/group__imgcodecs.html#gabbc7ef1aa2edfaa87772f1202d67e0ce")("ldr\_robertson.jpg", res\_robertson\_8bit)[cv.imwrite](../../d4/da8/group__imgcodecs.html#gabbc7ef1aa2edfaa87772f1202d67e0ce "../../d4/da8/group__imgcodecs.html#gabbc7ef1aa2edfaa87772f1202d67e0ce")("fusion\_mertens.jpg", res\_mertens\_8bit)## Results > (2)  > (3) Robertson:
opencv (L1) -> tutorial_py_table_of_contents_photo (L2) -> tutorial_py_hdr (L3)/n(Page path)
(1) Convert datatype to 8-bit and saveres\_debevec\_8bit = np.clip(res\_debevec\*255, 0, 255).astype('uint8')res\_robertson\_8bit = np.clip(res\_robertson\*255, 0, 255).astype('uint8')res\_mertens\_8bit = np.clip(res\_mertens\*255, 0, 255).astype('uint8')[cv.imwrite](../../d4/da8/group__imgcodecs.html#gabbc7ef1aa2edfaa87772f1202d67e0ce "../../d4/da8/group__imgcodecs.html#gabbc7ef1aa2edfaa87772f1202d67e0ce")("ldr\_debevec.jpg", res\_debevec\_8bit)[cv.imwrite](../../d4/da8/group__imgcodecs.html#gabbc7ef1aa2edfaa87772f1202d67e0ce "../../d4/da8/group__imgcodecs.html#gabbc7ef1aa2edfaa87772f1202d67e0ce")("ldr\_robertson.jpg", res\_robertson\_8bit)[cv.imwrite](../../d4/da8/group__imgcodecs.html#gabbc7ef1aa2edfaa87772f1202d67e0ce "../../d4/da8/group__imgcodecs.html#gabbc7ef1aa2edfaa87772f1202d67e0ce")("fusion\_mertens.jpg", res\_mertens\_8bit)## Results > (2)  > (3) Mertenes Fusion:
opencv (L1) -> tutorial_py_table_of_contents_photo (L2) -> tutorial_py_hdr (L3)/n(Page path)
(1) Convert datatype to 8-bit and saveres\_debevec\_8bit = np.clip(res\_debevec\*255, 0, 255).astype('uint8')res\_robertson\_8bit = np.clip(res\_robertson\*255, 0, 255).astype('uint8')res\_mertens\_8bit = np.clip(res\_mertens\*255, 0, 255).astype('uint8')[cv.imwrite](../../d4/da8/group__imgcodecs.html#gabbc7ef1aa2edfaa87772f1202d67e0ce "../../d4/da8/group__imgcodecs.html#gabbc7ef1aa2edfaa87772f1202d67e0ce")("ldr\_debevec.jpg", res\_debevec\_8bit)[cv.imwrite](../../d4/da8/group__imgcodecs.html#gabbc7ef1aa2edfaa87772f1202d67e0ce "../../d4/da8/group__imgcodecs.html#gabbc7ef1aa2edfaa87772f1202d67e0ce")("ldr\_robertson.jpg", res\_robertson\_8bit)[cv.imwrite](../../d4/da8/group__imgcodecs.html#gabbc7ef1aa2edfaa87772f1202d67e0ce "../../d4/da8/group__imgcodecs.html#gabbc7ef1aa2edfaa87772f1202d67e0ce")("fusion\_mertens.jpg", res\_mertens\_8bit)## Results > (2) Estimating Camera Response Function
opencv (L1) -> tutorial_py_table_of_contents_photo (L2) -> tutorial_py_hdr (L3)/n(Page path)
(1) Estimate camera response function (CRF)cal\_debevec = [cv.createCalibrateDebevec](../../d6/df5/group__photo__hdr.html#ga7fed9707ad5f2cc0e633888867109f90 "../../d6/df5/group__photo__hdr.html#ga7fed9707ad5f2cc0e633888867109f90")()crf\_debevec = cal\_debevec.process(img\_list, times=exposure\_times)hdr\_debevec = merge\_debevec.process(img\_list, times=exposure\_times.copy(), response=crf\_debevec.copy())cal\_robertson = [cv.createCalibrateRobertson](../../d6/df5/group__photo__hdr.html#gae77813a21cd351a596619e5ff013be5d "../../d6/df5/group__photo__hdr.html#gae77813a21cd351a596619e5ff013be5d")()crf\_robertson = cal\_robertson.process(img\_list, times=exposure\_times)hdr\_robertson = merge\_robertson.process(img\_list, times=exposure\_times.copy(), response=crf\_robertson.copy())The camera response function is represented by a 256-length vector for each color channel. For this sequence we got the following estimation: > (2) Additional Resources
opencv (L1) -> tutorial_py_table_of_contents_photo (L2) -> tutorial_py_hdr (L3)/n(Page path)
(1) Estimate camera response function (CRF)cal\_debevec = [cv.createCalibrateDebevec](../../d6/df5/group__photo__hdr.html#ga7fed9707ad5f2cc0e633888867109f90 "../../d6/df5/group__photo__hdr.html#ga7fed9707ad5f2cc0e633888867109f90")()crf\_debevec = cal\_debevec.process(img\_list, times=exposure\_times)hdr\_debevec = merge\_debevec.process(img\_list, times=exposure\_times.copy(), response=crf\_debevec.copy())cal\_robertson = [cv.createCalibrateRobertson](../../d6/df5/group__photo__hdr.html#gae77813a21cd351a596619e5ff013be5d "../../d6/df5/group__photo__hdr.html#gae77813a21cd351a596619e5ff013be5d")()crf\_robertson = cal\_robertson.process(img\_list, times=exposure\_times)hdr\_robertson = merge\_robertson.process(img\_list, times=exposure\_times.copy(), response=crf\_robertson.copy())The camera response function is represented by a 256-length vector for each color channel. For this sequence we got the following estimation: > (2) Exercises
opencv (L1) -> tutorial_py_table_of_contents_photo (L2) -> tutorial_py_non_local_means (L3)/n(Page path)
(1)  > (2) Image Denoising in OpenCV > (3) 2. [cv.fastNlMeansDenoisingMulti()](../../d1/d79/group__photo__denoise.html#gaf4421bf068c4d632ea7f0aa38e0bf172 "Modification of fastNlMeansDenoising function for images sequence where consecutive images have been ...")
opencv (L1) -> tutorial_py_table_of_contents_feature2d (L2) -> tutorial_py_surf_intro (L3)/n(Page path)
(1)  > (2) Theory
opencv (L1) -> tutorial_py_table_of_contents_feature2d (L2) -> tutorial_py_surf_intro (L3)/n(Page path)
(1)  > (2) SURF in OpenCV
opencv (L1) -> tutorial_py_table_of_contents_feature2d (L2) -> tutorial_py_surf_intro (L3)/n(Page path)
(1) Check present Hessian threshold>>> [print](../../df/d57/namespacecv_1_1dnn.html#a43417dcaeb3c1e2a09b9d948e234c366 "../../df/d57/namespacecv_1_1dnn.html#a43417dcaeb3c1e2a09b9d948e234c366")( surf.getHessianThreshold() )400.0# We set it to some 50000. Remember, it is just for representing in picture.# In actual cases, it is better to have a value 300-500>>> surf.setHessianThreshold(50000)# Again compute keypoints and check its number.>>> kp, des = surf.detectAndCompute(img,None)>>> [print](../../df/d57/namespacecv_1_1dnn.html#a43417dcaeb3c1e2a09b9d948e234c366 "../../df/d57/namespacecv_1_1dnn.html#a43417dcaeb3c1e2a09b9d948e234c366")( len(kp) )47 It is less than 50. Let's draw it on the image.
opencv (L1) -> tutorial_py_table_of_contents_feature2d (L2) -> tutorial_py_surf_intro (L3)/n(Page path)
(1) Check upright flag, if it False, set it to True>>> [print](../../df/d57/namespacecv_1_1dnn.html#a43417dcaeb3c1e2a09b9d948e234c366 "../../df/d57/namespacecv_1_1dnn.html#a43417dcaeb3c1e2a09b9d948e234c366")( surf.getUpright() )False>>> surf.setUpright(True)# Recompute the feature points and draw it>>> kp = surf.detect(img,None)>>> img2 = [cv.drawKeypoints](../../d4/d5d/group__features2d__draw.html#ga5d2bafe8c1c45289bc3403a40fb88920 "../../d4/d5d/group__features2d__draw.html#ga5d2bafe8c1c45289bc3403a40fb88920")(img,kp,None,(255,0,0),4)>>> plt.imshow(img2),plt.show() See the results below. All the orientations are shown in same direction. It is faster than previous. If you are working on cases where orientation is not a problem (like panorama stitching) etc, this is better.
opencv (L1) -> tutorial_py_table_of_contents_feature2d (L2) -> tutorial_py_surf_intro (L3)/n(Page path)
(1) Find size of descriptor>>> [print](../../df/d57/namespacecv_1_1dnn.html#a43417dcaeb3c1e2a09b9d948e234c366 "../../df/d57/namespacecv_1_1dnn.html#a43417dcaeb3c1e2a09b9d948e234c366")( surf.descriptorSize() )64# That means flag, "extended" is False.>>> surf.getExtended() False# So we make it to True to get 128-dim descriptors.>>> surf.setExtended(True)>>> kp, des = surf.detectAndCompute(img,None)>>> [print](../../df/d57/namespacecv_1_1dnn.html#a43417dcaeb3c1e2a09b9d948e234c366 "../../df/d57/namespacecv_1_1dnn.html#a43417dcaeb3c1e2a09b9d948e234c366")( surf.descriptorSize() )128>>> [print](../../df/d57/namespacecv_1_1dnn.html#a43417dcaeb3c1e2a09b9d948e234c366 "../../df/d57/namespacecv_1_1dnn.html#a43417dcaeb3c1e2a09b9d948e234c366")( des.shape )(47, 128) Remaining part is matching which we will do in another chapter. > (2) Additional Resources
opencv (L1) -> tutorial_py_table_of_contents_feature2d (L2) -> tutorial_py_surf_intro (L3)/n(Page path)
(1) Find size of descriptor>>> [print](../../df/d57/namespacecv_1_1dnn.html#a43417dcaeb3c1e2a09b9d948e234c366 "../../df/d57/namespacecv_1_1dnn.html#a43417dcaeb3c1e2a09b9d948e234c366")( surf.descriptorSize() )64# That means flag, "extended" is False.>>> surf.getExtended() False# So we make it to True to get 128-dim descriptors.>>> surf.setExtended(True)>>> kp, des = surf.detectAndCompute(img,None)>>> [print](../../df/d57/namespacecv_1_1dnn.html#a43417dcaeb3c1e2a09b9d948e234c366 "../../df/d57/namespacecv_1_1dnn.html#a43417dcaeb3c1e2a09b9d948e234c366")( surf.descriptorSize() )128>>> [print](../../df/d57/namespacecv_1_1dnn.html#a43417dcaeb3c1e2a09b9d948e234c366 "../../df/d57/namespacecv_1_1dnn.html#a43417dcaeb3c1e2a09b9d948e234c366")( des.shape )(47, 128) Remaining part is matching which we will do in another chapter. > (2) Exercises
opencv (L1) -> tutorial_py_table_of_contents_feature2d (L2) -> tutorial_py_matcher (L3)/n(Page path)
(1)  > (2) Basics of Brute-Force Matcher > (3) Brute-Force Matching with ORB Descriptors
opencv (L1) -> tutorial_py_table_of_contents_feature2d (L2) -> tutorial_py_matcher (L3)/n(Page path)
(1) create BFMatcher objectbf = [cv.BFMatcher](../../d3/da1/classcv_1_1BFMatcher.html "../../d3/da1/classcv_1_1BFMatcher.html")(cv.NORM\_HAMMING, crossCheck=True)# Match descriptors.matches = bf.match(des1,des2)# Sort them in the order of their distance.matches = sorted(matches, key = lambda x:x.distance)# Draw first 10 matches.img3 = [cv.drawMatches](../../d4/d5d/group__features2d__draw.html#ga62fbedb5206ab2faf411797e7055c90f "../../d4/d5d/group__features2d__draw.html#ga62fbedb5206ab2faf411797e7055c90f")(img1,kp1,img2,kp2,matches[:10],None,flags=cv.DrawMatchesFlags\_NOT\_DRAW\_SINGLE\_POINTS)plt.imshow(img3),plt.show() Below is the result I got: > (2)  > (3) Brute-Force Matching with SIFT Descriptors and Ratio Test
opencv (L1) -> tutorial_py_table_of_contents_feature2d (L2) -> tutorial_py_matcher (L3)/n(Page path)
(1) create BFMatcher objectbf = [cv.BFMatcher](../../d3/da1/classcv_1_1BFMatcher.html "../../d3/da1/classcv_1_1BFMatcher.html")(cv.NORM\_HAMMING, crossCheck=True)# Match descriptors.matches = bf.match(des1,des2)# Sort them in the order of their distance.matches = sorted(matches, key = lambda x:x.distance)# Draw first 10 matches.img3 = [cv.drawMatches](../../d4/d5d/group__features2d__draw.html#ga62fbedb5206ab2faf411797e7055c90f "../../d4/d5d/group__features2d__draw.html#ga62fbedb5206ab2faf411797e7055c90f")(img1,kp1,img2,kp2,matches[:10],None,flags=cv.DrawMatchesFlags\_NOT\_DRAW\_SINGLE\_POINTS)plt.imshow(img3),plt.show() Below is the result I got: > (2) FLANN based Matcher
opencv (L1) -> tutorial_py_table_of_contents_feature2d (L2) -> tutorial_py_features_harris (L3)/n(Page path)
(1)  > (2) Corner with SubPixel Accuracy
opencv (L1) -> tutorial_py_table_of_contents_feature2d (L2) -> tutorial_py_feature_homography (L3)/n(Page path)
(1)  > (2) Code
opencv (L1) -> tutorial_py_table_of_contents_feature2d (L2) -> tutorial_py_fast (L3)/n(Page path)
(1)  > (2) FAST Feature Detector in OpenCV
opencv (L1) -> tutorial_py_table_of_contents_core (L2) -> tutorial_py_image_arithmetics (L3)/n(Page path)
(1)  > (2) Goal
opencv (L1) -> tutorial_py_table_of_contents_core (L2) -> tutorial_py_image_arithmetics (L3)/n(Page path)
(1)  > (2) Image Addition
opencv (L1) -> tutorial_py_table_of_contents_core (L2) -> tutorial_py_image_arithmetics (L3)/n(Page path)
(1)  > (2) Image Blending
opencv (L1) -> tutorial_py_table_of_contents_core (L2) -> tutorial_py_image_arithmetics (L3)/n(Page path)
(1)  > (2) Bitwise Operations
opencv (L1) -> tutorial_py_table_of_contents_core (L2) -> tutorial_py_image_arithmetics (L3)/n(Page path)
(1) Load two imagesimg1 = [cv.imread](../../d4/da8/group__imgcodecs.html#ga288b8b3da0892bd651fce07b3bbd3a56 "../../d4/da8/group__imgcodecs.html#ga288b8b3da0892bd651fce07b3bbd3a56")('messi5.jpg')img2 = [cv.imread](../../d4/da8/group__imgcodecs.html#ga288b8b3da0892bd651fce07b3bbd3a56 "../../d4/da8/group__imgcodecs.html#ga288b8b3da0892bd651fce07b3bbd3a56")('opencv-logo-white.png')assert img1 is not None, "file could not be read, check with os.path.exists()"assert img2 is not None, "file could not be read, check with os.path.exists()"# I want to put logo on top-left corner, So I create a ROIrows,cols,channels = img2.shaperoi = img1[0:rows, 0:cols]# Now create a mask of logo and create its inverse mask alsoimg2gray = [cv.cvtColor](../../d8/d01/group__imgproc__color__conversions.html#ga397ae87e1288a81d2363b61574eb8cab "../../d8/d01/group__imgproc__color__conversions.html#ga397ae87e1288a81d2363b61574eb8cab")(img2,cv.COLOR\_BGR2GRAY)ret, mask = [cv.threshold](../../d7/d1b/group__imgproc__misc.html#gae8a4a146d1ca78c626a53577199e9c57 "../../d7/d1b/group__imgproc__misc.html#gae8a4a146d1ca78c626a53577199e9c57")(img2gray, 10, 255, cv.THRESH\_BINARY)mask\_inv = [cv.bitwise\_not](../../d2/de8/group__core__array.html#ga0002cf8b418479f4cb49a75442baee2f "../../d2/de8/group__core__array.html#ga0002cf8b418479f4cb49a75442baee2f")(mask)# Now black-out the area of logo in ROIimg1\_bg = [cv.bitwise\_and](../../d2/de8/group__core__array.html#ga60b4d04b251ba5eb1392c34425497e14 "../../d2/de8/group__core__array.html#ga60b4d04b251ba5eb1392c34425497e14")(roi,roi,mask = mask\_inv)# Take only region of logo from logo image.img2\_fg = [cv.bitwise\_and](../../d2/de8/group__core__array.html#ga60b4d04b251ba5eb1392c34425497e14 "../../d2/de8/group__core__array.html#ga60b4d04b251ba5eb1392c34425497e14")(img2,img2,mask = mask)# Put logo in ROI and modify the main imagedst = [cv.add](../../d2/de8/group__core__array.html#ga10ac1bfb180e2cfda1701d06c24fdbd6 "../../d2/de8/group__core__array.html#ga10ac1bfb180e2cfda1701d06c24fdbd6")(img1\_bg,img2\_fg)img1[0:rows, 0:cols ] = dst[cv.imshow](../../df/d24/group__highgui__opengl.html#gaae7e90aa3415c68dba22a5ff2cefc25d "../../df/d24/group__highgui__opengl.html#gaae7e90aa3415c68dba22a5ff2cefc25d")('res',img1)[cv.waitKey](../../d7/dfc/group__highgui.html#ga5628525ad33f52eab17feebcfba38bd7 "../../d7/dfc/group__highgui.html#ga5628525ad33f52eab17feebcfba38bd7")(0)[cv.destroyAllWindows](../../d7/dfc/group__highgui.html#ga6b7fc1c1a8960438156912027b38f481 "../../d7/dfc/group__highgui.html#ga6b7fc1c1a8960438156912027b38f481")() See the result below. Left image shows the mask we created. Right image shows the final result. For more understanding, display all the intermediate images in the above code, especially img1\_bg and img2\_fg. > (2) Additional Resources
opencv (L1) -> tutorial_py_table_of_contents_core (L2) -> tutorial_py_image_arithmetics (L3)/n(Page path)
(1) Load two imagesimg1 = [cv.imread](../../d4/da8/group__imgcodecs.html#ga288b8b3da0892bd651fce07b3bbd3a56 "../../d4/da8/group__imgcodecs.html#ga288b8b3da0892bd651fce07b3bbd3a56")('messi5.jpg')img2 = [cv.imread](../../d4/da8/group__imgcodecs.html#ga288b8b3da0892bd651fce07b3bbd3a56 "../../d4/da8/group__imgcodecs.html#ga288b8b3da0892bd651fce07b3bbd3a56")('opencv-logo-white.png')assert img1 is not None, "file could not be read, check with os.path.exists()"assert img2 is not None, "file could not be read, check with os.path.exists()"# I want to put logo on top-left corner, So I create a ROIrows,cols,channels = img2.shaperoi = img1[0:rows, 0:cols]# Now create a mask of logo and create its inverse mask alsoimg2gray = [cv.cvtColor](../../d8/d01/group__imgproc__color__conversions.html#ga397ae87e1288a81d2363b61574eb8cab "../../d8/d01/group__imgproc__color__conversions.html#ga397ae87e1288a81d2363b61574eb8cab")(img2,cv.COLOR\_BGR2GRAY)ret, mask = [cv.threshold](../../d7/d1b/group__imgproc__misc.html#gae8a4a146d1ca78c626a53577199e9c57 "../../d7/d1b/group__imgproc__misc.html#gae8a4a146d1ca78c626a53577199e9c57")(img2gray, 10, 255, cv.THRESH\_BINARY)mask\_inv = [cv.bitwise\_not](../../d2/de8/group__core__array.html#ga0002cf8b418479f4cb49a75442baee2f "../../d2/de8/group__core__array.html#ga0002cf8b418479f4cb49a75442baee2f")(mask)# Now black-out the area of logo in ROIimg1\_bg = [cv.bitwise\_and](../../d2/de8/group__core__array.html#ga60b4d04b251ba5eb1392c34425497e14 "../../d2/de8/group__core__array.html#ga60b4d04b251ba5eb1392c34425497e14")(roi,roi,mask = mask\_inv)# Take only region of logo from logo image.img2\_fg = [cv.bitwise\_and](../../d2/de8/group__core__array.html#ga60b4d04b251ba5eb1392c34425497e14 "../../d2/de8/group__core__array.html#ga60b4d04b251ba5eb1392c34425497e14")(img2,img2,mask = mask)# Put logo in ROI and modify the main imagedst = [cv.add](../../d2/de8/group__core__array.html#ga10ac1bfb180e2cfda1701d06c24fdbd6 "../../d2/de8/group__core__array.html#ga10ac1bfb180e2cfda1701d06c24fdbd6")(img1\_bg,img2\_fg)img1[0:rows, 0:cols ] = dst[cv.imshow](../../df/d24/group__highgui__opengl.html#gaae7e90aa3415c68dba22a5ff2cefc25d "../../df/d24/group__highgui__opengl.html#gaae7e90aa3415c68dba22a5ff2cefc25d")('res',img1)[cv.waitKey](../../d7/dfc/group__highgui.html#ga5628525ad33f52eab17feebcfba38bd7 "../../d7/dfc/group__highgui.html#ga5628525ad33f52eab17feebcfba38bd7")(0)[cv.destroyAllWindows](../../d7/dfc/group__highgui.html#ga6b7fc1c1a8960438156912027b38f481 "../../d7/dfc/group__highgui.html#ga6b7fc1c1a8960438156912027b38f481")() See the result below. Left image shows the mask we created. Right image shows the final result. For more understanding, display all the intermediate images in the above code, especially img1\_bg and img2\_fg. > (2) Exercises
opencv (L1) -> tutorial_py_table_of_contents_core (L2) -> tutorial_py_optimization (L3)/n(Page path)
(1)  > (2) Measuring Performance with OpenCV
opencv (L1) -> tutorial_py_table_of_contents_core (L2) -> tutorial_py_optimization (L3)/n(Page path)
(1) check if optimization is enabledIn [5]: [cv.useOptimized](../../db/de0/group__core__utils.html#gafa6d5d04eff341825573ec6c0aa6519f "../../db/de0/group__core__utils.html#gafa6d5d04eff341825573ec6c0aa6519f")()Out[5]: TrueIn [6]: %timeit res = [cv.medianBlur](../../d4/d86/group__imgproc__filter.html#ga564869aa33e58769b4469101aac458f9 "../../d4/d86/group__imgproc__filter.html#ga564869aa33e58769b4469101aac458f9")(img,49)10 loops, best of 3: 34.9 ms per loop# Disable itIn [7]: [cv.setUseOptimized](../../db/de0/group__core__utils.html#ga3c8487ea4449e550bc39575ede094c7a "../../db/de0/group__core__utils.html#ga3c8487ea4449e550bc39575ede094c7a")(False)In [8]: [cv.useOptimized](../../db/de0/group__core__utils.html#gafa6d5d04eff341825573ec6c0aa6519f "../../db/de0/group__core__utils.html#gafa6d5d04eff341825573ec6c0aa6519f")()Out[8]: FalseIn [9]: %timeit res = [cv.medianBlur](../../d4/d86/group__imgproc__filter.html#ga564869aa33e58769b4469101aac458f9 "../../d4/d86/group__imgproc__filter.html#ga564869aa33e58769b4469101aac458f9")(img,49)10 loops, best of 3: 64.1 ms per loop As you can see, optimized median filtering is2x faster than the unoptimized version. If you check its source, you can see that median filtering is SIMD optimized. So you can use this to enable optimization at the top of your code (remember it is enabled by default). > (2) Measuring Performance in IPython
opencv (L1) -> tutorial_py_table_of_contents_core (L2) -> tutorial_py_optimization (L3)/n(Page path)
(1) check if optimization is enabledIn [5]: [cv.useOptimized](../../db/de0/group__core__utils.html#gafa6d5d04eff341825573ec6c0aa6519f "../../db/de0/group__core__utils.html#gafa6d5d04eff341825573ec6c0aa6519f")()Out[5]: TrueIn [6]: %timeit res = [cv.medianBlur](../../d4/d86/group__imgproc__filter.html#ga564869aa33e58769b4469101aac458f9 "../../d4/d86/group__imgproc__filter.html#ga564869aa33e58769b4469101aac458f9")(img,49)10 loops, best of 3: 34.9 ms per loop# Disable itIn [7]: [cv.setUseOptimized](../../db/de0/group__core__utils.html#ga3c8487ea4449e550bc39575ede094c7a "../../db/de0/group__core__utils.html#ga3c8487ea4449e550bc39575ede094c7a")(False)In [8]: [cv.useOptimized](../../db/de0/group__core__utils.html#gafa6d5d04eff341825573ec6c0aa6519f "../../db/de0/group__core__utils.html#gafa6d5d04eff341825573ec6c0aa6519f")()Out[8]: FalseIn [9]: %timeit res = [cv.medianBlur](../../d4/d86/group__imgproc__filter.html#ga564869aa33e58769b4469101aac458f9 "../../d4/d86/group__imgproc__filter.html#ga564869aa33e58769b4469101aac458f9")(img,49)10 loops, best of 3: 64.1 ms per loop As you can see, optimized median filtering is2x faster than the unoptimized version. If you check its source, you can see that median filtering is SIMD optimized. So you can use this to enable optimization at the top of your code (remember it is enabled by default). > (2) More IPython magic commands
opencv (L1) -> tutorial_py_table_of_contents_core (L2) -> tutorial_py_optimization (L3)/n(Page path)
(1) check if optimization is enabledIn [5]: [cv.useOptimized](../../db/de0/group__core__utils.html#gafa6d5d04eff341825573ec6c0aa6519f "../../db/de0/group__core__utils.html#gafa6d5d04eff341825573ec6c0aa6519f")()Out[5]: TrueIn [6]: %timeit res = [cv.medianBlur](../../d4/d86/group__imgproc__filter.html#ga564869aa33e58769b4469101aac458f9 "../../d4/d86/group__imgproc__filter.html#ga564869aa33e58769b4469101aac458f9")(img,49)10 loops, best of 3: 34.9 ms per loop# Disable itIn [7]: [cv.setUseOptimized](../../db/de0/group__core__utils.html#ga3c8487ea4449e550bc39575ede094c7a "../../db/de0/group__core__utils.html#ga3c8487ea4449e550bc39575ede094c7a")(False)In [8]: [cv.useOptimized](../../db/de0/group__core__utils.html#gafa6d5d04eff341825573ec6c0aa6519f "../../db/de0/group__core__utils.html#gafa6d5d04eff341825573ec6c0aa6519f")()Out[8]: FalseIn [9]: %timeit res = [cv.medianBlur](../../d4/d86/group__imgproc__filter.html#ga564869aa33e58769b4469101aac458f9 "../../d4/d86/group__imgproc__filter.html#ga564869aa33e58769b4469101aac458f9")(img,49)10 loops, best of 3: 64.1 ms per loop As you can see, optimized median filtering is2x faster than the unoptimized version. If you check its source, you can see that median filtering is SIMD optimized. So you can use this to enable optimization at the top of your code (remember it is enabled by default). > (2) Performance Optimization Techniques
opencv (L1) -> tutorial_py_table_of_contents_core (L2) -> tutorial_py_optimization (L3)/n(Page path)
(1) check if optimization is enabledIn [5]: [cv.useOptimized](../../db/de0/group__core__utils.html#gafa6d5d04eff341825573ec6c0aa6519f "../../db/de0/group__core__utils.html#gafa6d5d04eff341825573ec6c0aa6519f")()Out[5]: TrueIn [6]: %timeit res = [cv.medianBlur](../../d4/d86/group__imgproc__filter.html#ga564869aa33e58769b4469101aac458f9 "../../d4/d86/group__imgproc__filter.html#ga564869aa33e58769b4469101aac458f9")(img,49)10 loops, best of 3: 34.9 ms per loop# Disable itIn [7]: [cv.setUseOptimized](../../db/de0/group__core__utils.html#ga3c8487ea4449e550bc39575ede094c7a "../../db/de0/group__core__utils.html#ga3c8487ea4449e550bc39575ede094c7a")(False)In [8]: [cv.useOptimized](../../db/de0/group__core__utils.html#gafa6d5d04eff341825573ec6c0aa6519f "../../db/de0/group__core__utils.html#gafa6d5d04eff341825573ec6c0aa6519f")()Out[8]: FalseIn [9]: %timeit res = [cv.medianBlur](../../d4/d86/group__imgproc__filter.html#ga564869aa33e58769b4469101aac458f9 "../../d4/d86/group__imgproc__filter.html#ga564869aa33e58769b4469101aac458f9")(img,49)10 loops, best of 3: 64.1 ms per loop As you can see, optimized median filtering is2x faster than the unoptimized version. If you check its source, you can see that median filtering is SIMD optimized. So you can use this to enable optimization at the top of your code (remember it is enabled by default). > (2) Additional Resources
opencv (L1) -> tutorial_py_table_of_contents_core (L2) -> tutorial_py_optimization (L3)/n(Page path)
(1) check if optimization is enabledIn [5]: [cv.useOptimized](../../db/de0/group__core__utils.html#gafa6d5d04eff341825573ec6c0aa6519f "../../db/de0/group__core__utils.html#gafa6d5d04eff341825573ec6c0aa6519f")()Out[5]: TrueIn [6]: %timeit res = [cv.medianBlur](../../d4/d86/group__imgproc__filter.html#ga564869aa33e58769b4469101aac458f9 "../../d4/d86/group__imgproc__filter.html#ga564869aa33e58769b4469101aac458f9")(img,49)10 loops, best of 3: 34.9 ms per loop# Disable itIn [7]: [cv.setUseOptimized](../../db/de0/group__core__utils.html#ga3c8487ea4449e550bc39575ede094c7a "../../db/de0/group__core__utils.html#ga3c8487ea4449e550bc39575ede094c7a")(False)In [8]: [cv.useOptimized](../../db/de0/group__core__utils.html#gafa6d5d04eff341825573ec6c0aa6519f "../../db/de0/group__core__utils.html#gafa6d5d04eff341825573ec6c0aa6519f")()Out[8]: FalseIn [9]: %timeit res = [cv.medianBlur](../../d4/d86/group__imgproc__filter.html#ga564869aa33e58769b4469101aac458f9 "../../d4/d86/group__imgproc__filter.html#ga564869aa33e58769b4469101aac458f9")(img,49)10 loops, best of 3: 64.1 ms per loop As you can see, optimized median filtering is2x faster than the unoptimized version. If you check its source, you can see that median filtering is SIMD optimized. So you can use this to enable optimization at the top of your code (remember it is enabled by default). > (2) Exercises
opencv (L1) -> tutorial_py_table_of_contents_core (L2) -> tutorial_py_basic_ops (L3)/n(Page path)
(1) accessing RED value>>> img.item(10,10,2)59# modifying RED value>>> img.itemset((10,10,2),100)>>> img.item(10,10,2)100## Accessing Image Properties > (2) Making Borders for Images (Padding)
opencv (L1) -> tutorial_py_table_of_contents_imgproc (L2) -> tutorial_py_grabcut (L3)/n(Page path)
(1)  > (2) Demo
opencv (L1) -> tutorial_py_table_of_contents_imgproc (L2) -> tutorial_py_grabcut (L3)/n(Page path)
(1) newmask is the mask image I manually labellednewmask = [cv.imread](../../d4/da8/group__imgcodecs.html#ga288b8b3da0892bd651fce07b3bbd3a56 "../../d4/da8/group__imgcodecs.html#ga288b8b3da0892bd651fce07b3bbd3a56")('newmask.png', cv.IMREAD\_GRAYSCALE)assert newmask is not None, "file could not be read, check with os.path.exists()"# wherever it is marked white (sure foreground), change mask=1# wherever it is marked black (sure background), change mask=0mask[newmask == 0] = 0mask[newmask == 255] = 1mask, bgdModel, fgdModel = [cv.grabCut](../../d3/d47/group__imgproc__segmentation.html#ga909c1dda50efcbeaa3ce126be862b37f "../../d3/d47/group__imgproc__segmentation.html#ga909c1dda50efcbeaa3ce126be862b37f")(img,mask,None,bgdModel,fgdModel,5,cv.GC\_INIT\_WITH\_MASK)mask = np.where((mask==2)|(mask==0),0,1).astype('uint8')img = img\*mask[:,:,np.newaxis]plt.imshow(img),plt.colorbar(),plt.show() See the result below: > (2) Additional Resources
opencv (L1) -> tutorial_py_table_of_contents_imgproc (L2) -> tutorial_py_grabcut (L3)/n(Page path)
(1) newmask is the mask image I manually labellednewmask = [cv.imread](../../d4/da8/group__imgcodecs.html#ga288b8b3da0892bd651fce07b3bbd3a56 "../../d4/da8/group__imgcodecs.html#ga288b8b3da0892bd651fce07b3bbd3a56")('newmask.png', cv.IMREAD\_GRAYSCALE)assert newmask is not None, "file could not be read, check with os.path.exists()"# wherever it is marked white (sure foreground), change mask=1# wherever it is marked black (sure background), change mask=0mask[newmask == 0] = 0mask[newmask == 255] = 1mask, bgdModel, fgdModel = [cv.grabCut](../../d3/d47/group__imgproc__segmentation.html#ga909c1dda50efcbeaa3ce126be862b37f "../../d3/d47/group__imgproc__segmentation.html#ga909c1dda50efcbeaa3ce126be862b37f")(img,mask,None,bgdModel,fgdModel,5,cv.GC\_INIT\_WITH\_MASK)mask = np.where((mask==2)|(mask==0),0,1).astype('uint8')img = img\*mask[:,:,np.newaxis]plt.imshow(img),plt.colorbar(),plt.show() See the result below: > (2) Exercises
opencv (L1) -> tutorial_py_table_of_contents_imgproc (L2) -> tutorial_py_template_matching (L3)/n(Page path)
(1)  > (2) Template Matching in OpenCV
opencv (L1) -> tutorial_py_table_of_contents_imgproc (L2) -> tutorial_py_watershed (L3)/n(Page path)
(1)  > (2) Code
opencv (L1) -> tutorial_py_table_of_contents_imgproc (L2) -> tutorial_py_watershed (L3)/n(Page path)
(1) noise removalkernel = np.ones((3,3),np.uint8)opening = [cv.morphologyEx](../../d4/d86/group__imgproc__filter.html#ga67493776e3ad1a3df63883829375201f "../../d4/d86/group__imgproc__filter.html#ga67493776e3ad1a3df63883829375201f")(thresh,cv.MORPH\_OPEN,kernel, iterations = 2)# sure background areasure\_bg = [cv.dilate](../../d4/d86/group__imgproc__filter.html#ga4ff0f3318642c4f469d0e11f242f3b6c "../../d4/d86/group__imgproc__filter.html#ga4ff0f3318642c4f469d0e11f242f3b6c")(opening,kernel,iterations=3)# Finding sure foreground areadist\_transform = [cv.distanceTransform](../../d7/d1b/group__imgproc__misc.html#ga25c259e7e2fa2ac70de4606ea800f12f "../../d7/d1b/group__imgproc__misc.html#ga25c259e7e2fa2ac70de4606ea800f12f")(opening,cv.DIST\_L2,5)ret, sure\_fg = [cv.threshold](../../d7/d1b/group__imgproc__misc.html#gae8a4a146d1ca78c626a53577199e9c57 "../../d7/d1b/group__imgproc__misc.html#gae8a4a146d1ca78c626a53577199e9c57")(dist\_transform,0.7\*dist\_transform.max(),255,0)# Finding unknown regionsure\_fg = np.uint8(sure\_fg)unknown = [cv.subtract](../../d2/de8/group__core__array.html#gaa0f00d98b4b5edeaeb7b8333b2de353b "../../d2/de8/group__core__array.html#gaa0f00d98b4b5edeaeb7b8333b2de353b")(sure\_bg,sure\_fg) See the result. In the thresholded image, we get some regions of coins which we are sure of coins and they are detached now. (In some cases, you may be interested in only foreground segmentation, not in separating the mutually touching objects. In that case, you need not use distance transform, just erosion is sufficient. Erosion is just another method to extract sure foreground area, that's all.)
opencv (L1) -> tutorial_py_table_of_contents_imgproc (L2) -> tutorial_py_watershed (L3)/n(Page path)
(1) Marker labellingret, markers = [cv.connectedComponents](../../d3/dc0/group__imgproc__shape.html#gac2718a64ade63475425558aa669a943a "../../d3/dc0/group__imgproc__shape.html#gac2718a64ade63475425558aa669a943a")(sure\_fg)# Add one to all labels so that sure background is not 0, but 1markers = markers+1# Now, mark the region of unknown with zeromarkers[unknown==255] = 0 See the result shown in JET colormap. The dark blue region shows unknown region. Sure coins are colored with different values. Remaining area which are sure background are shown in lighter blue compared to unknown region. > (2) Additional Resources
opencv (L1) -> tutorial_py_table_of_contents_imgproc (L2) -> tutorial_py_watershed (L3)/n(Page path)
(1) Marker labellingret, markers = [cv.connectedComponents](../../d3/dc0/group__imgproc__shape.html#gac2718a64ade63475425558aa669a943a "../../d3/dc0/group__imgproc__shape.html#gac2718a64ade63475425558aa669a943a")(sure\_fg)# Add one to all labels so that sure background is not 0, but 1markers = markers+1# Now, mark the region of unknown with zeromarkers[unknown==255] = 0 See the result shown in JET colormap. The dark blue region shows unknown region. Sure coins are colored with different values. Remaining area which are sure background are shown in lighter blue compared to unknown region. > (2) Exercises
opencv (L1) -> tutorial_py_table_of_contents_imgproc (L2) -> tutorial_py_morphological_ops (L3)/n(Page path)
(1) Rectangular Kernel>>> [cv.getStructuringElement](../../d4/d86/group__imgproc__filter.html#gac342a1bb6eabf6f55c803b09268e36dc "../../d4/d86/group__imgproc__filter.html#gac342a1bb6eabf6f55c803b09268e36dc")(cv.MORPH\_RECT,(5,5))array([[1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1]], dtype=uint8)# Elliptical Kernel>>> [cv.getStructuringElement](../../d4/d86/group__imgproc__filter.html#gac342a1bb6eabf6f55c803b09268e36dc "../../d4/d86/group__imgproc__filter.html#gac342a1bb6eabf6f55c803b09268e36dc")(cv.MORPH\_ELLIPSE,(5,5))array([[0, 0, 1, 0, 0], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [0, 0, 1, 0, 0]], dtype=uint8)# Cross-shaped Kernel>>> [cv.getStructuringElement](../../d4/d86/group__imgproc__filter.html#gac342a1bb6eabf6f55c803b09268e36dc "../../d4/d86/group__imgproc__filter.html#gac342a1bb6eabf6f55c803b09268e36dc")(cv.MORPH\_CROSS,(5,5))array([[0, 0, 1, 0, 0], [0, 0, 1, 0, 0], [1, 1, 1, 1, 1], [0, 0, 1, 0, 0], [0, 0, 1, 0, 0]], dtype=uint8) ## Additional Resources > (2) Exercises
opencv (L1) -> tutorial_py_table_of_contents_imgproc (L2) -> tutorial_py_geometric_transformations (L3)/n(Page path)
(1)  > (2) Transformations > (3) Scaling
opencv (L1) -> tutorial_py_table_of_contents_imgproc (L2) -> tutorial_py_table_of_contents_contours (L3) -> tutorial_py_contour_properties (L4)/n(Page path)
(1)  > (2) 1. Aspect Ratio
opencv (L1) -> tutorial_py_table_of_contents_imgproc (L2) -> tutorial_py_table_of_contents_contours (L3) -> tutorial_py_contour_features (L4)/n(Page path)
(1)  > (2) 1. Moments
opencv (L1) -> tutorial_py_table_of_contents_imgproc (L2) -> tutorial_py_table_of_contents_contours (L3) -> tutorial_py_contours_hierarchy (L4)/n(Page path)
(1)  > (2) Theory > (3) Hierarchy Representation in OpenCV
opencv (L1) -> tutorial_py_table_of_contents_imgproc (L2) -> tutorial_py_table_of_contents_contours (L3) -> tutorial_py_contours_more_functions (L4)/n(Page path)
(1)  > (2) Theory and Code > (3) 1. Convexity Defects
opencv (L1) -> tutorial_py_table_of_contents_imgproc (L2) -> tutorial_py_table_of_contents_contours (L3) -> tutorial_py_contours_more_functions (L4)/n(Page path)
(1)  > (2) Theory and Code > (3) 3. Match Shapes
opencv (L1) -> tutorial_py_table_of_contents_imgproc (L2) -> tutorial_py_pyramids (L3)/n(Page path)
(1)  > (2) Image Blending using Pyramids
opencv (L1) -> tutorial_py_table_of_contents_imgproc (L2) -> tutorial_py_table_of_contents_histograms (L3) -> tutorial_py_histogram_backprojection (L4)/n(Page path)
(1)  > (2) Algorithm in Numpy
opencv (L1) -> tutorial_py_table_of_contents_imgproc (L2) -> tutorial_py_table_of_contents_histograms (L3) -> tutorial_py_histogram_backprojection (L4)/n(Page path)
(1)  > (2) Backprojection in OpenCV
opencv (L1) -> tutorial_py_table_of_contents_imgproc (L2) -> tutorial_py_houghlines (L3)/n(Page path)
(1) Hough Transform in OpenCV > (2) Probabilistic Hough Transform
opencv (L1) -> tutorial_py_table_of_contents_imgproc (L2) -> tutorial_py_table_of_contents_transforms (L3) -> tutorial_py_fourier_transform (L4)/n(Page path)
(1)  > (2) Fourier Transform in OpenCV
opencv (L1) -> tutorial_py_table_of_contents_imgproc (L2) -> tutorial_py_table_of_contents_transforms (L3) -> tutorial_py_fourier_transform (L4)/n(Page path)
(1) Performance Optimization of DFT > (2) Why Laplacian is a High Pass Filter?
opencv (L1) -> tutorial_py_table_of_contents_imgproc (L2) -> tutorial_py_table_of_contents_transforms (L3) -> tutorial_py_fourier_transform (L4)/n(Page path)
(1) Performance Optimization of DFT > (2) Additional Resources
opencv (L1) -> tutorial_py_table_of_contents_imgproc (L2) -> tutorial_py_table_of_contents_transforms (L3) -> tutorial_py_fourier_transform (L4)/n(Page path)
(1) Performance Optimization of DFT > (2) Exercises
opencv (L1) -> tutorial_py_table_of_contents_imgproc (L2) -> tutorial_py_thresholding (L3)/n(Page path)
(1)  > (2) Simple Thresholding
opencv (L1) -> tutorial_py_table_of_contents_imgproc (L2) -> tutorial_py_thresholding (L3)/n(Page path)
(1)  > (2) Adaptive Thresholding
opencv (L1) -> tutorial_py_table_of_contents_imgproc (L2) -> tutorial_py_thresholding (L3)/n(Page path)
(1)  > (2) Otsu's Binarization > (3) How does Otsu's Binarization work?
opencv (L1) -> tutorial_py_table_of_contents_setup (L2) -> tutorial_py_setup_in_fedora (L3)/n(Page path)
(1)  > (2) Installing OpenCV from source > (3) Compulsory Dependencies
opencv (L1) -> tutorial_py_table_of_contents_gui (L2) -> tutorial_py_trackbar (L3)/n(Page path)
(1)  > (2) Code Demo
opencv (L1) -> tutorial_py_table_of_contents_gui (L2) -> tutorial_py_drawing_functions (L3)/n(Page path)
(1)  > (2) Code > (3) Drawing Line
opencv (L1) -> tutorial_py_table_of_contents_gui (L2) -> tutorial_py_video_display (L3)/n(Page path)
(1)  > (2) Capture Video from Camera
opencv (L1) -> tutorial_py_table_of_contents_gui (L2) -> tutorial_py_mouse_handling (L3)/n(Page path)
(1)  > (2) Simple Demo